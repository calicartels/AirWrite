{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyORDHdy0BRBeSAi3wlriHxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calicartels/AirWrite/blob/main/Research_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Basic imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 2. Setting up directory and mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Making sure we're in /content before creating directories\n",
        "%cd /content\n",
        "\n",
        "# 4. Cleaning if directories exist and cloning the original directories\n",
        "!rm -rf flow_matching\n",
        "!git clone https://github.com/facebookresearch/flow_matching.git\n",
        "\n",
        "# 5. Install package\n",
        "%cd /content/flow_matching\n",
        "!pip install -e .\n",
        "\n",
        "# 6. Set up image example directory\n",
        "%cd /content/flow_matching/examples/image\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 7. Create output directory and __init__.py files\n",
        "!mkdir -p output_dir\n",
        "!touch /content/flow_matching/examples/image/models/__init__.py\n",
        "!touch /content/flow_matching/examples/image/training/__init__.py\n",
        "\n",
        "# 8. Copy your saved model from Drive\n",
        "!cp -r \"/content/drive/MyDrive/flow_matching_model/.\" \"/content/flow_matching/examples/image/output_dir/\"\n",
        "\n",
        "# 9. Clean and reset Python path\n",
        "import sys\n",
        "sys.path = ['/content/flow_matching/examples/image', '/content/flow_matching'] + sys.path\n",
        "\n",
        "# 10. Try imports\n",
        "try:\n",
        "    # Import model components\n",
        "    from models.model_configs import instantiate_model\n",
        "    from training.eval_loop import CFGScaledModel\n",
        "    print(\"Model imports successful!\")\n",
        "\n",
        "    # Import flow_matching components\n",
        "    import flow_matching\n",
        "    from flow_matching.path import MixtureDiscreteProbPath\n",
        "    from flow_matching.path.scheduler import PolynomialConvexScheduler\n",
        "    from flow_matching.solver.ode_solver import ODESolver\n",
        "    print(\"All imports successful!\")\n",
        "\n",
        "    # Load model configuration\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "\n",
        "    checkpoint_path = Path(\"/content/flow_matching/examples/image/output_dir/checkpoint-199.pth\")\n",
        "    args_filepath = checkpoint_path.parent / 'args.json'\n",
        "\n",
        "    with open(args_filepath, 'r') as f:\n",
        "        args_dict = json.load(f)\n",
        "\n",
        "    print(\"Configuration loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"\\nCurrent working directory:\", os.getcwd())\n",
        "    print(\"\\nPython path:\", sys.path)\n",
        "    print(\"\\nDirectory contents:\")\n",
        "    !ls -R /content/flow_matching/examples/image/models\n",
        "    !ls -R /content/flow_matching/flow_matching/path"
      ],
      "metadata": {
        "id": "XGKQRvj_vd7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import torch.serialization\n",
        "torch.serialization.add_safe_globals([Namespace])\n",
        "\n",
        "# 1. Initialize the model\n",
        "model = instantiate_model(\n",
        "    architechture=args_dict['dataset'],\n",
        "    is_discrete='discrete_flow_matching' in args_dict and args_dict['discrete_flow_matching'],\n",
        "    use_ema=args_dict['use_ema']\n",
        ")\n",
        "\n",
        "# 2. Load checkpoint (with weights_only=True to address the warning)\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
        "model.load_state_dict(checkpoint[\"model\"])\n",
        "model.eval()  # Set to evaluation mode\n",
        "model.to(device)\n",
        "\n",
        "# 3. Setup for generation\n",
        "batch_size = 16  # Number of images to generate\n",
        "sample_resolution = 32  # CIFAR10 resolution\n",
        "cfg_weighted_model = CFGScaledModel(model=model)\n",
        "\n",
        "# 4. Generate images\n",
        "try:\n",
        "    x_0 = torch.randn([batch_size, 3, sample_resolution, sample_resolution], dtype=torch.float32, device=device)\n",
        "    solver = ODESolver(velocity_model=cfg_weighted_model)\n",
        "\n",
        "    # Get ODE options from args_dict or use defaults\n",
        "    ode_opts = args_dict.get('ode_options', {})\n",
        "    step_size = ode_opts.get('step_size', 0.05)  # Default step size if not specified\n",
        "\n",
        "    synthetic_samples = solver.sample(\n",
        "        time_grid=torch.tensor([0.0, 1.0], device=device),\n",
        "        x_init=x_0,\n",
        "        method=args_dict.get('ode_method', 'euler'),  # Default to euler if not specified\n",
        "        step_size=step_size,\n",
        "        atol=ode_opts.get('atol', 1e-5),\n",
        "        rtol=ode_opts.get('rtol', 1e-5),\n",
        "        label=torch.tensor(list(range(batch_size)), device=device),\n",
        "        cfg_scale=args_dict.get('cfg_scale', 1.0)\n",
        "    )\n",
        "\n",
        "    # Scale images to [0, 1] range\n",
        "    synthetic_samples = torch.clamp(synthetic_samples * 0.5 + 0.5, min=0.0, max=1.0)\n",
        "\n",
        "    # Visualize generated images\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(synthetic_samples[i].cpu().permute(1, 2, 0).numpy())\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Generation error: {e}\")\n",
        "    print(\"\\nargs_dict contents:\")\n",
        "    print(json.dumps(args_dict, indent=2))  # Print args_dict for debugging"
      ],
      "metadata": {
        "id": "OacNCuU30vlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** the shitty image quality is because of a higer FiD value, close to 5.5.\n",
        "\n",
        "** Also i trained on only 200 epochs and CiFar needs close to 921 for an ideal train and 3000 for the perfect model."
      ],
      "metadata": {
        "id": "UvcayXnnNXRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Basic imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import Namespace\n",
        "import torch.serialization\n",
        "from models.model_configs import instantiate_model\n",
        "from training.eval_loop import CFGScaledModel\n",
        "from flow_matching.solver.ode_solver import ODESolver\n",
        "import gc\n",
        "\n",
        "# Clear GPU memory and cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Add Namespace to safe globals\n",
        "torch.serialization.add_safe_globals([Namespace])\n",
        "\n",
        "# Setup paths and load model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint_path = Path(\"/content/flow_matching/examples/image/output_dir/checkpoint-199.pth\")\n",
        "args_filepath = checkpoint_path.parent / 'args.json'\n",
        "\n",
        "with open(args_filepath, 'r') as f:\n",
        "    args_dict = json.load(f)\n",
        "\n",
        "# Initialize and load model\n",
        "model = instantiate_model(\n",
        "    architechture=args_dict['dataset'],\n",
        "    is_discrete='discrete_flow_matching' in args_dict and args_dict['discrete_flow_matching'],\n",
        "    use_ema=args_dict['use_ema']\n",
        ")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
        "model.load_state_dict(checkpoint[\"model\"])\n",
        "del checkpoint\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Modified FGSM attack for Flow Matching model\n",
        "def fgsm_attack(model, image, label, epsilon=0.03):\n",
        "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Create timestep tensor (assuming t=0.5 for attack)\n",
        "    timesteps = torch.ones((image.shape[0],), device=device) * 0.5\n",
        "\n",
        "    # Forward pass with required arguments\n",
        "    with torch.enable_grad():\n",
        "        output = model(perturbed_image, timesteps, extra={'y': label})\n",
        "        # For Flow Matching models, we'll use the difference between output and input as loss\n",
        "        loss = F.mse_loss(output, perturbed_image)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Create adversarial example\n",
        "        adversarial_image = perturbed_image + epsilon * perturbed_image.grad.sign()\n",
        "        adversarial_image = torch.clamp(adversarial_image, -1, 1)\n",
        "\n",
        "    return adversarial_image.detach()\n",
        "\n",
        "# Generate samples and create adversarial examples\n",
        "try:\n",
        "    # Setup generation\n",
        "    batch_size = 8\n",
        "    sample_resolution = 32\n",
        "    cfg_weighted_model = CFGScaledModel(model=model)\n",
        "\n",
        "    # Generate original samples\n",
        "    x_0 = torch.randn([batch_size, 3, sample_resolution, sample_resolution],\n",
        "                      dtype=torch.float32, device=device)\n",
        "    solver = ODESolver(velocity_model=cfg_weighted_model)\n",
        "\n",
        "    # Get ODE options\n",
        "    ode_opts = args_dict.get('ode_options', {})\n",
        "    step_size = ode_opts.get('step_size', 0.05)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        synthetic_samples = solver.sample(\n",
        "            time_grid=torch.tensor([0.0, 1.0], device=device),\n",
        "            x_init=x_0,\n",
        "            method=args_dict.get('ode_method', 'heun2'),  # Using heun2 as specified in args\n",
        "            step_size=step_size,\n",
        "            atol=ode_opts.get('atol', 1e-5),\n",
        "            rtol=ode_opts.get('rtol', 1e-5),\n",
        "            label=torch.tensor(list(range(batch_size)), device=device),\n",
        "            cfg_scale=args_dict.get('cfg_scale', 0.0)  # Using cfg_scale from args\n",
        "        )\n",
        "\n",
        "    # Scale to [0, 1] range\n",
        "    synthetic_samples = torch.clamp(synthetic_samples * 0.5 + 0.5, min=0.0, max=1.0)\n",
        "\n",
        "    # Create adversarial examples\n",
        "    adversarial_samples = []\n",
        "    for i in range(batch_size):\n",
        "        image = synthetic_samples[i].unsqueeze(0)\n",
        "        label = torch.tensor([i], device=device)\n",
        "        adv_image = fgsm_attack(model, image, label)\n",
        "        adversarial_samples.append(adv_image)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    adversarial_samples = torch.cat(adversarial_samples)\n",
        "\n",
        "    # Move to CPU for visualization\n",
        "    synthetic_samples = synthetic_samples.cpu()\n",
        "    adversarial_samples = adversarial_samples.cpu()\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i in range(batch_size):\n",
        "        # Original image\n",
        "        plt.subplot(2, batch_size, i + 1)\n",
        "        plt.imshow(synthetic_samples[i].permute(1, 2, 0).numpy())\n",
        "        plt.title('Original')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Adversarial image\n",
        "        plt.subplot(2, batch_size, i + batch_size + 1)\n",
        "        plt.imshow(adversarial_samples[i].permute(1, 2, 0).numpy())\n",
        "        plt.title('Adversarial')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Generation/Attack error: {e}\")\n",
        "    print(\"\\nargs_dict contents:\")\n",
        "    print(json.dumps(args_dict, indent=2))\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "n-EcEtFr893g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load and prepare ResNet classifier\n",
        "classifier = models.resnet50(weights=ResNet50_Weights.DEFAULT).to(device)\n",
        "classifier.eval()\n",
        "\n",
        "def improved_fgsm_attack(model, image, target_class_idx, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Improved FGSM attack with targeted misclassification\n",
        "    \"\"\"\n",
        "    # Clone and track gradients for the original image\n",
        "    image = image.clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Create target tensor\n",
        "    target = torch.tensor([target_class_idx], device=device)\n",
        "\n",
        "    # Multiple attack iterations\n",
        "    for _ in range(5):\n",
        "        # Reset gradients\n",
        "        if image.grad is not None:\n",
        "            image.grad.zero_()\n",
        "\n",
        "        # Forward pass through classifier\n",
        "        output = classifier(image)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Ensure we have gradients\n",
        "        if image.grad is None:\n",
        "            print(\"No gradients were computed!\")\n",
        "            continue\n",
        "\n",
        "        # Create perturbation\n",
        "        perturbation = epsilon * image.grad.sign()\n",
        "\n",
        "        # Update image\n",
        "        with torch.no_grad():\n",
        "            image = torch.clamp(image + perturbation, -1, 1).detach()\n",
        "            image.requires_grad_(True)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Try the improved attack\n",
        "try:\n",
        "    # Take one sample\n",
        "    original = synthetic_samples[0].unsqueeze(0).to(device)\n",
        "\n",
        "    # Preprocess original image to match ResNet's expected input\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    # Resize to 224x224 (ResNet's expected input size)\n",
        "    resize = transforms.Resize((224, 224))\n",
        "    original = resize(original)\n",
        "    original = normalize(original)\n",
        "\n",
        "    # Get original prediction\n",
        "    with torch.no_grad():\n",
        "        orig_output = classifier(original)\n",
        "        orig_class = orig_output.argmax().item()\n",
        "        orig_conf = F.softmax(orig_output, dim=1).max().item()\n",
        "\n",
        "    print(f\"\\nOriginal Classification:\")\n",
        "    print(f\"Class: {orig_class}, Confidence: {orig_conf:.4f}\")\n",
        "\n",
        "    # Try different epsilon values and target classes\n",
        "    epsilon_values = [0.05, 0.1, 0.15, 0.2]\n",
        "    target_classes = [\n",
        "        404,  # airliner\n",
        "        751,  # wing\n",
        "        895,  # aircraft carrier\n",
        "        627,  # helicopter\n",
        "    ]\n",
        "\n",
        "    for epsilon in epsilon_values:\n",
        "        print(f\"\\nTrying epsilon = {epsilon}\")\n",
        "        for target_class in target_classes:\n",
        "            print(f\"\\nTrying to misclassify as class {target_class}\")\n",
        "\n",
        "            # Generate adversarial example\n",
        "            adv_image = improved_fgsm_attack(model, original, target_class, epsilon=epsilon)\n",
        "\n",
        "            # Get adversarial prediction\n",
        "            with torch.no_grad():\n",
        "                adv_output = classifier(adv_image)\n",
        "                adv_class = adv_output.argmax().item()\n",
        "                adv_conf = F.softmax(adv_output, dim=1).max().item()\n",
        "\n",
        "            print(f\"New Classification - Class: {adv_class}, Confidence: {adv_conf:.4f}\")\n",
        "\n",
        "            # Only show visualization if classification changed\n",
        "            if adv_class != orig_class:\n",
        "                plt.figure(figsize=(15, 5))\n",
        "\n",
        "                # Denormalize images for visualization\n",
        "                denormalize = transforms.Compose([\n",
        "                    transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
        "                    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
        "                ])\n",
        "\n",
        "                # Original\n",
        "                plt.subplot(1, 3, 1)\n",
        "                vis_orig = denormalize(original[0]).cpu().detach()\n",
        "                plt.imshow(vis_orig.permute(1, 2, 0).numpy().clip(0, 1))\n",
        "                plt.title(f'Original\\nClass: {orig_class}\\nConf: {orig_conf:.4f}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Adversarial\n",
        "                plt.subplot(1, 3, 2)\n",
        "                vis_adv = denormalize(adv_image[0]).cpu().detach()\n",
        "                plt.imshow(vis_adv.permute(1, 2, 0).numpy().clip(0, 1))\n",
        "                plt.title(f'Adversarial\\nClass: {adv_class}\\nConf: {adv_conf:.4f}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Perturbation\n",
        "                plt.subplot(1, 3, 3)\n",
        "                perturbation = (adv_image - original)[0].cpu().detach()\n",
        "                plt.imshow(np.abs(perturbation.permute(1, 2, 0).numpy()).mean(axis=2), cmap='viridis')\n",
        "                plt.colorbar()\n",
        "                plt.title('Perturbation Magnitude')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Attack error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "40i6BkV5DOnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hemSsBEGDPu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack(model, image, target_class_idx, epsilon=0.03, alpha=0.01, num_iter=40):\n",
        "    \"\"\"\n",
        "    PGD Attack implementation\n",
        "    Args:\n",
        "        model: target model\n",
        "        image: original image\n",
        "        target_class_idx: target class for adversarial example\n",
        "        epsilon: maximum perturbation\n",
        "        alpha: step size\n",
        "        num_iter: number of iterations\n",
        "    \"\"\"\n",
        "    # Initialize\n",
        "    perturbed_image = image.clone().detach()\n",
        "    # Add small random noise to start\n",
        "    perturbed_image = perturbed_image + torch.empty_like(image).uniform_(-epsilon/2, epsilon/2)\n",
        "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
        "\n",
        "    # Create target tensor\n",
        "    target = torch.tensor([target_class_idx], device=device)\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        perturbed_image.requires_grad = True\n",
        "\n",
        "        # Forward pass\n",
        "        output = classifier(perturbed_image)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Get gradient\n",
        "        grad = perturbed_image.grad.data\n",
        "\n",
        "        # Update image - gradient ascent since we want to maximize loss\n",
        "        adv_image = perturbed_image + alpha * grad.sign()\n",
        "\n",
        "        # Project back to epsilon ball and valid image space\n",
        "        eta = torch.clamp(adv_image - image, min=-epsilon, max=epsilon)\n",
        "        perturbed_image = torch.clamp(image + eta, min=-1, max=1).detach()\n",
        "\n",
        "    return perturbed_image\n",
        "\n",
        "# Compare FGSM and PGD attacks\n",
        "try:\n",
        "    # Take one sample\n",
        "    original = synthetic_samples[0].unsqueeze(0).to(device)\n",
        "\n",
        "    # Get original prediction\n",
        "    with torch.no_grad():\n",
        "        orig_output = classifier(original)\n",
        "        orig_class = orig_output.argmax().item()\n",
        "        orig_conf = F.softmax(orig_output, dim=1).max().item()\n",
        "\n",
        "    print(f\"\\nOriginal Classification:\")\n",
        "    print(f\"Class: {orig_class}, Confidence: {orig_conf:.4f}\")\n",
        "\n",
        "    # Test parameters\n",
        "    epsilons = [0.03, 0.05, 0.1]\n",
        "    target_classes = [404, 895]  # Using classes that worked well with FGSM\n",
        "\n",
        "    for epsilon in epsilons:\n",
        "        print(f\"\\nTesting epsilon = {epsilon}\")\n",
        "        for target_class in target_classes:\n",
        "            print(f\"\\nTarget class: {target_class}\")\n",
        "\n",
        "            # Generate FGSM adversarial example\n",
        "            fgsm_image = improved_fgsm_attack(model, original, target_class, epsilon=epsilon)\n",
        "\n",
        "            # Generate PGD adversarial example\n",
        "            pgd_image = pgd_attack(model, original, target_class, epsilon=epsilon)\n",
        "\n",
        "            # Get predictions\n",
        "            with torch.no_grad():\n",
        "                # FGSM predictions\n",
        "                fgsm_output = classifier(fgsm_image)\n",
        "                fgsm_class = fgsm_output.argmax().item()\n",
        "                fgsm_conf = F.softmax(fgsm_output, dim=1).max().item()\n",
        "\n",
        "                # PGD predictions\n",
        "                pgd_output = classifier(pgd_image)\n",
        "                pgd_class = pgd_output.argmax().item()\n",
        "                pgd_conf = F.softmax(pgd_output, dim=1).max().item()\n",
        "\n",
        "            print(f\"FGSM - New Class: {fgsm_class}, Confidence: {fgsm_conf:.4f}\")\n",
        "            print(f\"PGD  - New Class: {pgd_class}, Confidence: {pgd_conf:.4f}\")\n",
        "\n",
        "            # Visualize results\n",
        "            plt.figure(figsize=(20, 5))\n",
        "\n",
        "            # Original\n",
        "            plt.subplot(1, 4, 1)\n",
        "            plt.imshow(original[0].cpu().detach().permute(1, 2, 0).numpy())\n",
        "            plt.title(f'Original\\nClass: {orig_class}\\nConf: {orig_conf:.4f}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # FGSM\n",
        "            plt.subplot(1, 4, 2)\n",
        "            plt.imshow(fgsm_image[0].cpu().detach().permute(1, 2, 0).numpy())\n",
        "            plt.title(f'FGSM\\nClass: {fgsm_class}\\nConf: {fgsm_conf:.4f}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # PGD\n",
        "            plt.subplot(1, 4, 3)\n",
        "            plt.imshow(pgd_image[0].cpu().detach().permute(1, 2, 0).numpy())\n",
        "            plt.title(f'PGD\\nClass: {pgd_class}\\nConf: {pgd_conf:.4f}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Perturbation comparison\n",
        "            plt.subplot(1, 4, 4)\n",
        "            fgsm_pert = torch.norm((fgsm_image - original)[0], dim=0).cpu().detach()\n",
        "            pgd_pert = torch.norm((pgd_image - original)[0], dim=0).cpu().detach()\n",
        "            plt.plot(fgsm_pert.mean(dim=1), label='FGSM')\n",
        "            plt.plot(pgd_pert.mean(dim=1), label='PGD')\n",
        "            plt.title('Perturbation Magnitude\\nper Row')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Attack error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "clTq6n8o89yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqafxrrF89vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjz9YjoE89s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "QppHSHZT78qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "class PerturbationFlowTracker:\n",
        "    def __init__(self, original_image: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Initialize tracker with original image\n",
        "        Args:\n",
        "            original_image: Original image tensor [1, C, H, W]\n",
        "        \"\"\"\n",
        "        self.original_image = original_image\n",
        "        self.flow_history = []\n",
        "        self.perturbation_history = []\n",
        "        self.intermediate_images = []\n",
        "\n",
        "    def track_step(self, current_image: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Track changes from one step to the next\n",
        "        Args:\n",
        "            current_image: Current perturbed image\n",
        "        \"\"\"\n",
        "        # Store intermediate image\n",
        "        self.intermediate_images.append(current_image.clone().detach().cpu())\n",
        "\n",
        "        # Calculate perturbation from original\n",
        "        perturbation = (current_image - self.original_image).detach().cpu()\n",
        "        self.perturbation_history.append(perturbation)\n",
        "\n",
        "        # Calculate flow if we have at least 2 images\n",
        "        if len(self.intermediate_images) > 1:\n",
        "            prev_image = self.intermediate_images[-2]\n",
        "            current_image = self.intermediate_images[-1]\n",
        "\n",
        "            # Calculate optical flow between consecutive images\n",
        "            flow = self._calculate_flow(prev_image, current_image)\n",
        "            self.flow_history.append(flow)\n",
        "\n",
        "    def _calculate_flow(self, prev_image: torch.Tensor, current_image: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"Calculate optical flow between two images\"\"\"\n",
        "        # Convert to numpy arrays in correct format for cv2\n",
        "        prev_np = (prev_image[0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        curr_np = (current_image[0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "\n",
        "        # Convert to grayscale\n",
        "        prev_gray = cv2.cvtColor(prev_np, cv2.COLOR_RGB2GRAY)\n",
        "        curr_gray = cv2.cvtColor(curr_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Calculate flow\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, curr_gray,\n",
        "            None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "        )\n",
        "        return flow\n",
        "\n",
        "    def visualize_flow_step(self, step: int):\n",
        "        \"\"\"Visualize flow at a specific step\"\"\"\n",
        "        if step >= len(self.flow_history):\n",
        "            raise ValueError(f\"Step {step} not available. Only have {len(self.flow_history)} steps.\")\n",
        "\n",
        "        flow = self.flow_history[step]\n",
        "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(self.original_image[0].permute(1, 2, 0).cpu())\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Current perturbed image\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(self.intermediate_images[step+1][0].permute(1, 2, 0).cpu())\n",
        "        plt.title(f'Perturbed Image (Step {step+1})')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Flow visualization\n",
        "        plt.subplot(133)\n",
        "        plt.quiver(flow[..., 0], flow[..., 1])\n",
        "        plt.title('Flow Field')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def create_flow_animation(self, save_path: str = None):\n",
        "        \"\"\"Create animation of the flow progression\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        def update(frame):\n",
        "            for ax in axes:\n",
        "                ax.clear()\n",
        "\n",
        "            # Original image\n",
        "            axes[0].imshow(self.original_image[0].permute(1, 2, 0).cpu())\n",
        "            axes[0].set_title('Original Image')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Current perturbed image\n",
        "            axes[1].imshow(self.intermediate_images[frame][0].permute(1, 2, 0).cpu())\n",
        "            axes[1].set_title(f'Perturbed Image (Step {frame})')\n",
        "            axes[1].axis('off')\n",
        "\n",
        "            if frame > 0:\n",
        "                # Flow visualization\n",
        "                flow = self.flow_history[frame-1]\n",
        "                axes[2].quiver(flow[..., 0], flow[..., 1])\n",
        "                axes[2].set_title('Flow Field')\n",
        "                axes[2].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "        anim = FuncAnimation(\n",
        "            fig, update,\n",
        "            frames=len(self.intermediate_images),\n",
        "            interval=200\n",
        "        )\n",
        "\n",
        "        if save_path:\n",
        "            anim.save(save_path, writer='pillow')\n",
        "\n",
        "        plt.close()\n",
        "        return anim\n",
        "\n",
        "# Modified PGD attack to use the tracker\n",
        "def pgd_attack_with_tracking(model, image, target_class_idx, epsilon=0.1, alpha=0.02, num_iter=100):\n",
        "    \"\"\"PGD Attack with perturbation tracking\"\"\"\n",
        "    # Initialize tracker\n",
        "    tracker = PerturbationFlowTracker(image)\n",
        "\n",
        "    # Initialize attack\n",
        "    perturbed_image = image.clone().detach()\n",
        "    perturbed_image = perturbed_image + torch.empty_like(image).uniform_(-epsilon/2, epsilon/2)\n",
        "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
        "\n",
        "    # Track initial state\n",
        "    tracker.track_step(perturbed_image)\n",
        "\n",
        "    target = torch.tensor([target_class_idx], device=device)\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        perturbed_image.requires_grad_(True)\n",
        "\n",
        "        output = classifier(perturbed_image)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # Zero gradients\n",
        "        if perturbed_image.grad is not None:\n",
        "            perturbed_image.grad.data.zero_()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Update image\n",
        "        adv_image = perturbed_image + alpha * perturbed_image.grad.sign()\n",
        "        eta = torch.clamp(adv_image - image, min=-epsilon, max=epsilon)\n",
        "        perturbed_image = torch.clamp(image + eta, min=-1, max=1).detach()\n",
        "\n",
        "        # Track this step\n",
        "        tracker.track_step(perturbed_image)\n",
        "\n",
        "    return perturbed_image, tracker"
      ],
      "metadata": {
        "id": "GwSdZaiU72L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Take one sample\n",
        "    original = synthetic_samples[0].unsqueeze(0).to(device)\n",
        "\n",
        "    # Define target class\n",
        "    target_class_idx = 404  # airliner\n",
        "\n",
        "    # Get original prediction\n",
        "    with torch.no_grad():\n",
        "        orig_output = classifier(original)\n",
        "        orig_class = orig_output.argmax().item()\n",
        "        orig_conf = F.softmax(orig_output, dim=1).max().item()\n",
        "\n",
        "    print(f\"\\nOriginal Classification:\")\n",
        "    print(f\"Class: {orig_class}, Confidence: {orig_conf:.4f}\")\n",
        "\n",
        "    # Generate adversarial example with tracking\n",
        "    print(\"\\nGenerating adversarial example...\")\n",
        "    adv_image, tracker = pgd_attack_with_tracking(\n",
        "        model=model,\n",
        "        image=original,\n",
        "        target_class_idx=target_class_idx,\n",
        "        epsilon=0.1,\n",
        "        alpha=0.02,\n",
        "        num_iter=100\n",
        "    )\n",
        "\n",
        "    # Get adversarial prediction\n",
        "    with torch.no_grad():\n",
        "        adv_output = classifier(adv_image)\n",
        "        adv_class = adv_output.argmax().item()\n",
        "        adv_conf = F.softmax(adv_output, dim=1).max().item()\n",
        "\n",
        "    print(f\"\\nAdversarial Classification:\")\n",
        "    print(f\"Class: {adv_class}, Confidence: {adv_conf:.4f}\")\n",
        "\n",
        "    # Visualize steps\n",
        "    print(\"\\nVisualizing attack progression...\")\n",
        "    steps_to_show = [0,1,2,3,4,5,6,7,8,9,10,11,12,1]  # Show progression at different points\n",
        "    for step in steps_to_show:\n",
        "        if step < len(tracker.flow_history):\n",
        "            print(f\"\\nShowing step {step}\")\n",
        "            tracker.visualize_flow_step(step)\n",
        "\n",
        "    # Create and save animation\n",
        "    print(\"\\nCreating animation...\")\n",
        "    animation = tracker.create_flow_animation('perturbation_flow.gif')\n",
        "    print(\"Animation saved as 'perturbation_flow.gif'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "p-0Ak68e89ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "\n",
        "class EnhancedFeatureExtractor:\n",
        "    def __init__(self, model):\n",
        "        \"\"\"Enhanced feature extractor with improved tracking capabilities\"\"\"\n",
        "        self.model = model\n",
        "        self.features = {}\n",
        "        self.hooks = []\n",
        "        self.imagenet_labels = self._load_imagenet_labels()\n",
        "\n",
        "        # Track more layers for better analysis\n",
        "        self.target_layers = [\n",
        "            'layer1.2.conv3',    # Early visual features\n",
        "            'layer2.3.conv3',    # Mid-level features\n",
        "            'layer3.5.conv3',    # Higher-level features\n",
        "            'layer4.2.conv3',    # Final features\n",
        "            'avgpool'            # Global features\n",
        "        ]\n",
        "\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _load_imagenet_labels(self):\n",
        "        \"\"\"Load ImageNet class labels\"\"\"\n",
        "        # Default ImageNet classes if file not available\n",
        "        return [f\"Class_{i}\" for i in range(1000)]\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def hook_fn(layer_name):\n",
        "            def forward_hook(module, input, output):\n",
        "                self.features[layer_name] = output.detach()\n",
        "            return forward_hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if any(target in name for target in self.target_layers):\n",
        "                hook = module.register_forward_hook(hook_fn(name))\n",
        "                self.hooks.append(hook)\n",
        "                print(f\"Registered hook for layer: {name}\")\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Get features for input x with proper preprocessing\"\"\"\n",
        "        self.features.clear()\n",
        "\n",
        "        # Ensure input is properly normalized\n",
        "        if x.min() < -1 or x.max() > 1:\n",
        "            x = x / 255.0 if x.max() > 1 else x\n",
        "            x = x * 2 - 1  # Scale to [-1, 1]\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(x)\n",
        "\n",
        "        return self.features.copy()\n",
        "\n",
        "    def calculate_attention(self, features, layer_name='layer4.2.conv3'):\n",
        "        \"\"\"Improved Grad-CAM style attention calculation\"\"\"\n",
        "        feature_map = features[layer_name]\n",
        "        B, C, H, W = feature_map.shape\n",
        "\n",
        "        # Calculate channel importance weights using GAP\n",
        "        weights = F.adaptive_avg_pool2d(feature_map, 1)  # [B, C, 1, 1]\n",
        "\n",
        "        # Compute weighted sum of feature maps\n",
        "        cam = torch.zeros((B, H, W), device=feature_map.device)\n",
        "        for i in range(C):\n",
        "            cam += weights[:, i, 0, 0][:, None, None] * feature_map[:, i, :, :]\n",
        "\n",
        "        # Apply ReLU to focus on features that positively influence the decision\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize\n",
        "        B = cam.shape[0]\n",
        "        cam_min = cam.view(B, -1).min(1)[0].view(B, 1, 1)\n",
        "        cam_max = cam.view(B, -1).max(1)[0].view(B, 1, 1)\n",
        "        cam = (cam - cam_min) / (cam_max - cam_min + 1e-8)\n",
        "\n",
        "        # Resize to input size\n",
        "        cam = F.interpolate(\n",
        "            cam.unsqueeze(1),\n",
        "            size=(224, 224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        return cam\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Clean up hooks\"\"\"\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []"
      ],
      "metadata": {
        "id": "fPccyvATOsRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedFeatureTracker:\n",
        "    def __init__(self, model, save_history=True):\n",
        "        \"\"\"Enhanced feature tracker with improved visualization and tracking\"\"\"\n",
        "        self.feature_extractor = EnhancedFeatureExtractor(model)\n",
        "        self.save_history = save_history\n",
        "\n",
        "        # History tracking\n",
        "        self.feature_history = []\n",
        "        self.attention_history = []\n",
        "        self.prediction_history = []\n",
        "        self.feature_changes = []\n",
        "\n",
        "    def track_step(self, image, step):\n",
        "        \"\"\"Track features, attention, and predictions for each step\"\"\"\n",
        "        # Get features\n",
        "        features = self.feature_extractor.get_features(image)\n",
        "\n",
        "        # Calculate attention\n",
        "        attention = self.feature_extractor.calculate_attention(features)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            output = self.feature_extractor.model(image)\n",
        "            predictions = F.softmax(output, dim=1)\n",
        "\n",
        "        # Store history if enabled\n",
        "        if self.save_history:\n",
        "            self.feature_history.append(features)\n",
        "            self.attention_history.append(attention)\n",
        "            self.prediction_history.append(predictions)\n",
        "\n",
        "            # Calculate feature changes if we have previous features\n",
        "            if len(self.feature_history) > 1:\n",
        "                changes = self.calculate_feature_changes(\n",
        "                    self.feature_history[-2],\n",
        "                    features\n",
        "                )\n",
        "                self.feature_changes.append(changes)\n",
        "\n",
        "        return features, attention, predictions\n",
        "\n",
        "    def calculate_feature_changes(self, prev_features, curr_features):\n",
        "        \"\"\"Calculate changes in feature representations\"\"\"\n",
        "        changes = {}\n",
        "        for layer_name in self.feature_extractor.target_layers:\n",
        "            if layer_name in prev_features and layer_name in curr_features:\n",
        "                prev = prev_features[layer_name]\n",
        "                curr = curr_features[layer_name]\n",
        "\n",
        "                # Normalize features\n",
        "                prev_flat = F.normalize(prev.view(prev.size(0), -1), p=2, dim=1)\n",
        "                curr_flat = F.normalize(curr.view(curr.size(0), -1), p=2, dim=1)\n",
        "\n",
        "                # Calculate cosine distance\n",
        "                similarity = F.cosine_similarity(prev_flat, curr_flat)\n",
        "                changes[layer_name] = (1 - similarity).mean().item()\n",
        "\n",
        "        return changes\n",
        "\n",
        "    def visualize_step_detailed(self, step):\n",
        "      \"\"\"Enhanced visualization of model behavior at each step\"\"\"\n",
        "      if step >= len(self.feature_history):\n",
        "          raise ValueError(f\"Step {step} not available\")\n",
        "\n",
        "      features = self.feature_history[step]\n",
        "      attention = self.attention_history[step]\n",
        "      predictions = self.prediction_history[step]\n",
        "\n",
        "      # Create figure with subplots\n",
        "      fig = plt.figure(figsize=(20, 12))\n",
        "      gs = plt.GridSpec(3, 4, figure=fig)\n",
        "\n",
        "      # 1. Feature maps from different layers\n",
        "      for idx, layer_name in enumerate(self.feature_extractor.target_layers[:-1]):\n",
        "          ax = fig.add_subplot(gs[0, idx])\n",
        "          feature_map = features[layer_name]\n",
        "          feature_vis = torch.mean(feature_map, dim=1)[0].cpu()\n",
        "\n",
        "          im = ax.imshow(feature_vis.numpy(), cmap='viridis')\n",
        "          ax.set_title(f'Layer {layer_name}\\nFeatures')\n",
        "          ax.axis('off')\n",
        "          plt.colorbar(im, ax=ax)\n",
        "\n",
        "      # 2. Attention visualization\n",
        "      ax = fig.add_subplot(gs[1, 0])\n",
        "      attention_vis = attention[0, 0].cpu().numpy()\n",
        "      im = ax.imshow(attention_vis, cmap='hot')\n",
        "      ax.set_title('Attention Map')\n",
        "      ax.axis('off')\n",
        "      plt.colorbar(im, ax=ax)\n",
        "\n",
        "      # 3. Top predictions\n",
        "      ax = fig.add_subplot(gs[1, 1])\n",
        "      values, indices = predictions[0].cpu().topk(5)\n",
        "      ax.bar(range(5), values.numpy())\n",
        "      ax.set_xticks(range(5))\n",
        "      ax.set_xticklabels([f\"Class {idx}\" for idx in indices.numpy()], rotation=45)\n",
        "      ax.set_title('Top 5 Predictions')\n",
        "\n",
        "      # 4. Feature changes if available\n",
        "      if step > 0 and self.feature_changes:\n",
        "          ax = fig.add_subplot(gs[1, 2])\n",
        "          changes = self.feature_changes[step-1]\n",
        "          layers = list(changes.keys())\n",
        "          values = list(changes.values())\n",
        "          ax.bar(range(len(changes)), values)\n",
        "          ax.set_xticks(range(len(changes)))\n",
        "          ax.set_xticklabels(layers, rotation=45)\n",
        "          ax.set_title('Feature Changes')\n",
        "\n",
        "      # 5. Combined visualization - Fixed\n",
        "      ax = fig.add_subplot(gs[1, 3])\n",
        "      early_features = torch.mean(features[self.feature_extractor.target_layers[0]], dim=1)[0]\n",
        "      # Resize early features to match attention map size\n",
        "      early_features_resized = F.interpolate(\n",
        "          early_features.unsqueeze(0).unsqueeze(0),\n",
        "          size=(224, 224),\n",
        "          mode='bilinear',\n",
        "          align_corners=False\n",
        "      )[0, 0].cpu().numpy()\n",
        "\n",
        "      # Normalize early features\n",
        "      early_features_resized = (early_features_resized - early_features_resized.min()) / \\\n",
        "                              (early_features_resized.max() - early_features_resized.min() + 1e-8)\n",
        "\n",
        "      combined = early_features_resized * attention_vis\n",
        "      im = ax.imshow(combined, cmap='viridis')\n",
        "      ax.set_title('Features + Attention')\n",
        "      ax.axis('off')\n",
        "      plt.colorbar(im, ax=ax)\n",
        "\n",
        "      plt.tight_layout()\n",
        "      return fig\n",
        "\n",
        "    def create_progression_animation(self, save_path=None):\n",
        "        \"\"\"Create animation showing the progression of the attack\"\"\"\n",
        "        raise NotImplementedError(\"Animation functionality to be implemented\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.feature_extractor.remove_hooks()\n",
        "        if self.save_history:\n",
        "            self.feature_history = []\n",
        "            self.attention_history = []\n",
        "            self.prediction_history = []\n",
        "            self.feature_changes = []"
      ],
      "metadata": {
        "id": "2s0J_R_U2JI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack_with_enhanced_tracking(model, classifier, image, target_class_idx,\n",
        "                                    epsilon=0.1, alpha=0.02, num_iter=100):\n",
        "    \"\"\"PGD Attack with enhanced tracking and visualization\"\"\"\n",
        "    # Initialize tracker\n",
        "    tracker = EnhancedFeatureTracker(classifier)\n",
        "\n",
        "    # Initialize attack\n",
        "    perturbed_image = image.clone().detach()\n",
        "    perturbed_image = perturbed_image + torch.empty_like(image).uniform_(-epsilon/2, epsilon/2)\n",
        "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
        "\n",
        "    # Track initial state\n",
        "    tracker.track_step(perturbed_image, 0)\n",
        "\n",
        "    target = torch.tensor([target_class_idx], device=device)\n",
        "    loss_history = []\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        perturbed_image.requires_grad_(True)\n",
        "\n",
        "        output = classifier(perturbed_image)\n",
        "        loss = -F.cross_entropy(output, target)\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if perturbed_image.grad is not None:\n",
        "            perturbed_image.grad.data.zero_()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        grad = perturbed_image.grad.data\n",
        "        grad_norm = torch.norm(grad, p=float('inf'))\n",
        "        normalized_grad = grad / (grad_norm + 1e-10)\n",
        "\n",
        "        adv_image = perturbed_image + alpha * normalized_grad\n",
        "        eta = torch.clamp(adv_image - image, min=-epsilon, max=epsilon)\n",
        "        perturbed_image = torch.clamp(image + eta, min=-1, max=1).detach()\n",
        "\n",
        "        # Track every step instead of just 10% intervals\n",
        "        tracker.track_step(perturbed_image, i+1)\n",
        "\n",
        "        # Print progress less frequently\n",
        "        if (i + 1) % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                current_output = classifier(perturbed_image)\n",
        "                current_prob = F.softmax(current_output, dim=1)[0][target_class_idx].item()\n",
        "            print(f\"Step {i+1}/{num_iter}: Target class probability = {current_prob:.4f}\")\n",
        "\n",
        "    return perturbed_image, tracker, loss_history\n",
        "\n",
        "# Usage example\n",
        "def run_attack_analysis():\n",
        "    try:\n",
        "        original = synthetic_samples[0].unsqueeze(0).to(device)\n",
        "        target_class_idx = 404  # airliner\n",
        "\n",
        "        print(\"\\nStarting adversarial attack with enhanced tracking...\")\n",
        "        adv_image, tracker, loss_history = pgd_attack_with_enhanced_tracking(\n",
        "            model=model,\n",
        "            classifier=classifier,\n",
        "            image=original,\n",
        "            target_class_idx=target_class_idx,\n",
        "            epsilon=0.1,\n",
        "            alpha=0.02,\n",
        "            num_iter=100\n",
        "        )\n",
        "\n",
        "        # Show fewer steps to avoid memory issues\n",
        "        steps_to_show = [0, 20, 40, 60, 80]  # Adjusted step intervals\n",
        "        for step in steps_to_show:\n",
        "            print(f\"\\nVisualizing step {step}...\")\n",
        "            fig = tracker.visualize_step_detailed(step)\n",
        "            plt.show()\n",
        "            plt.close(fig)\n",
        "\n",
        "        # Plot loss history\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(loss_history)\n",
        "        plt.title('Attack Loss History')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # Create final comparison\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(original[0].cpu().permute(1, 2, 0).numpy())\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(adv_image[0].cpu().permute(1, 2, 0).numpy())\n",
        "        plt.title('Adversarial Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(133)\n",
        "        perturbation = (adv_image - original)[0].cpu().permute(1, 2, 0).numpy()\n",
        "        plt.imshow(np.abs(perturbation) * 5)\n",
        "        plt.title('Perturbation (x5)')\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Cleanup\n",
        "        tracker.cleanup()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during attack analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    run_attack_analysis()"
      ],
      "metadata": {
        "id": "fdcqyDH22H5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sAp1m-dI-sgA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}